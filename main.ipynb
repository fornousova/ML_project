{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea459643-ae23-41b7-80ad-e252355d9534",
   "metadata": {},
   "source": [
    "# Model pro rozpoznávání různých druhů geometrických útvarů"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1052558-83f9-47fc-87f6-50c6b8d77026",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Tento notebook je zaměřen na rozpoznávání geometrických útvarů pomocí modelu MobileNetV2\n",
    "\n",
    "Nakonec jsem se rozhodla místo Google Colab pracovat v prostředí **Jupyter Notebook** (chtěla bych v něm pracovat i na svém maturitním projektu a ráda bych si na něj zvykla). \n",
    "\n",
    "Původně jsem plánovala vytrénovat model pro klasifikaci druhů koktejlů. Tento pokus však nebyl úspěšný – i přes několik experimentů a změn jsem nebyla schopna dosáhnout vyšší validační přesnosti než 30 %. Důvodem byla pravděpodobně vizuální podobnost mezi jednotlivými koktejly (např. tvary sklenic), což model zmátlo, protože se neměl na čem naučit jednoznačné rozdíly.\n",
    "\n",
    "Rozhodla jsem se tedy přejít na jinou úlohu a stáhla jsem si připravený dataset obsahující obrázky geometrických tvarů (kruhy, čtverce, hvězdy, pentagony, heptagony a trojúhelníky). Abych se vyhnula zavádějícím vzorům, zkontrolovala jsem dataset a ujistila se, že obrázky jsou konzistentní – stejné rozlišení a bez rušivých prvků. Navíc jsem použila augmentaci dat (např. rotace nebo změny velikosti), aby model lépe rozlišoval mezi podobnými tvary.\n",
    "\n",
    "Pro tuto úlohu jsem po dlouhém zkoušení nakonec zvolila předtrénovaný model **MobileNetV2**, který jsem dále upravila pro klasifikaci těchto šesti kategorií. Model se mi podařilo úspěšně natrénovat a nyní funguje i predikce nových obrázků. Největší problém se objevuje hlavně při rozpoznávání pentagonů, a poté i u heptagonů a čtverců, které si model kvůli jejich podobnosti někdy zaměňuje. Bylo by možné na tomto problému ještě zapracovat, například zlepšením kvality dat nebo přidáním dalších vzorů pro učení.\n",
    "\n",
    "I přesto jsem s výsledkem spokojená, protože model dokáže úlohu řešit s dobrou přesností a tento notebook slouží jako dokumentace celého procesu.\n",
    "\n",
    "---\n",
    "\n",
    "**Struktura notebooku**:\n",
    "1. Instalace a import potřebných knihoven.\n",
    "2. Předzpracování dat a vytvoření datového generátoru.\n",
    "3. Definice modelu MobileNetV2.\n",
    "4. Trénink modelu a vyhodnocení jeho přesnosti.\n",
    "5. Predikce na nových obrázcích."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82856ccf-ecf2-4443-959f-29804f176692",
   "metadata": {},
   "source": [
    "## Instalace knihoven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efaeda1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.24.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.24.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras) (13.9.2)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow keras \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65adffb-1cbf-452c-86f0-980ecbc622de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c18bdde-65f2-4011-bac9-5ccfdd21b6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec95ff-9f68-413b-8b80-0ac2884b977d",
   "metadata": {},
   "source": [
    "## Importování knihoven a nastavení konstant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da62d41-f29c-410c-8564-33fb4d00fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Nastavení konstant\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45e7e2-2bce-413a-8e00-c59042f458ab",
   "metadata": {},
   "source": [
    "## Předzpracování obrázků"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e18958af-4a56-46c5-aa7e-e81882c07814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE)) # Změna velikosti obrázku na požadovanou velikost (IMG_SIZE x IMG_SIZE)\n",
    "    \n",
    "    # Normalizace obrázku: převede hodnoty pixelů z rozsahu [0, 255] na [0, 1]\n",
    "    image = image / 255.0  # Normalizace na interval [0, 1]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05b9b3-3eef-44c5-be13-e237c326e2c3",
   "metadata": {},
   "source": [
    "## Augmentace obrázků"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5886d3a-c5e9-40f5-a85e-3866439675da",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Použití 20 % obrázků pro validaci (rozdělení dat na trénovací a validační část)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4a73ba-8ebb-4f59-b0d5-5afa96be8944",
   "metadata": {},
   "source": [
    "## Příprava trénovacích a validačních dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7615a3-3d69-4564-ae3a-cd1ff62214a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9600 images belonging to 6 classes.\n",
      "Found 2400 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f70362-cc22-4b14-966f-356761edae7b",
   "metadata": {},
   "source": [
    "## Definice modelu pomocí MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2fe9ca1-f33c-444b-b8fa-90b890eb7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(6, activation='softmax')  # Změňte počet tříd podle datasetu\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b423280-a68c-43bc-97b6-c2cb674bcb1e",
   "metadata": {},
   "source": [
    "## Kompilace modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a137aa9-ad59-4c0e-9906-3ce2937f1dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_128            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20480</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,310,784</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_128            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20480\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m1,310,784\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,569,158</span> (13.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,569,158\u001b[0m (13.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,311,174</span> (5.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,311,174\u001b[0m (5.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb132758-3601-4663-bbdb-5b06e39741df",
   "metadata": {},
   "source": [
    "## Trénování modelu a nastavení callbacků"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd420a1-d6bd-41ae-b8a4-f19a90fc23b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.3558 - loss: 1.4524"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 95ms/step - accuracy: 0.3560 - loss: 1.4521 - val_accuracy: 0.6350 - val_loss: 0.9425 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 91ms/step - accuracy: 0.4737 - loss: 1.1720 - val_accuracy: 0.6696 - val_loss: 0.8767 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 93ms/step - accuracy: 0.4951 - loss: 1.1471 - val_accuracy: 0.6896 - val_loss: 0.7458 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 97ms/step - accuracy: 0.5092 - loss: 1.0945 - val_accuracy: 0.6650 - val_loss: 0.8284 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 101ms/step - accuracy: 0.5311 - loss: 1.0461 - val_accuracy: 0.7171 - val_loss: 0.7425 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 100ms/step - accuracy: 0.5308 - loss: 1.0463 - val_accuracy: 0.7075 - val_loss: 0.7434 - learning_rate: 0.0010\n",
      "Epoch 7/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 98ms/step - accuracy: 0.5319 - loss: 1.0171 - val_accuracy: 0.7083 - val_loss: 0.7187 - learning_rate: 0.0010\n",
      "Epoch 8/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 100ms/step - accuracy: 0.5396 - loss: 1.0138 - val_accuracy: 0.7167 - val_loss: 0.7094 - learning_rate: 0.0010\n",
      "Epoch 9/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 100ms/step - accuracy: 0.5711 - loss: 0.9904 - val_accuracy: 0.7296 - val_loss: 0.6884 - learning_rate: 0.0010\n",
      "Epoch 10/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 104ms/step - accuracy: 0.5674 - loss: 0.9944 - val_accuracy: 0.7092 - val_loss: 0.7343 - learning_rate: 0.0010\n",
      "Epoch 11/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 104ms/step - accuracy: 0.5845 - loss: 0.9898 - val_accuracy: 0.7175 - val_loss: 0.7075 - learning_rate: 0.0010\n",
      "Epoch 12/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 105ms/step - accuracy: 0.5745 - loss: 0.9820 - val_accuracy: 0.7287 - val_loss: 0.6872 - learning_rate: 0.0010\n",
      "Epoch 13/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - accuracy: 0.5859 - loss: 0.9650 - val_accuracy: 0.7354 - val_loss: 0.7067 - learning_rate: 0.0010\n",
      "Epoch 14/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 113ms/step - accuracy: 0.5864 - loss: 0.9647 - val_accuracy: 0.7454 - val_loss: 0.6670 - learning_rate: 0.0010\n",
      "Epoch 15/15\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 115ms/step - accuracy: 0.5848 - loss: 0.9538 - val_accuracy: 0.7400 - val_loss: 0.6656 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 15.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Předčasné zastavení\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=5,  # Počet epoch, po kterých dojde k zastavení, pokud nedojde ke zlepšení\n",
    "                               verbose=1,   # Sledování validační ztráty (loss)\n",
    "                               mode='min',  # Hledání minimální hodnoty ztráty\n",
    "                               restore_best_weights=True)  # Obnovení nejlepšího modelu\n",
    "\n",
    "\n",
    "# Snížení rychlosti učení\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                 patience=3, \n",
    "                                 factor=0.5, \n",
    "                                 min_lr=1e-6)\n",
    "\n",
    "# Trénování modelu\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=15, # Počet epoch trénování (měnit!)\n",
    "    callbacks=[lr_reduction, early_stopping],  # Předání callbacků\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bb9c57-fe22-40f1-9bf7-fa0cbe18a997",
   "metadata": {},
   "source": [
    "## Vyhodnocení modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d432c08-5917-4eb7-b5dc-62fd78b76eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - accuracy: 0.7302 - loss: 0.6826\n",
      "Validation accuracy: 73.17%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a9bc2-0485-4890-96ca-735be55fb50a",
   "metadata": {},
   "source": [
    "## Predikce nového obrázku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a110cf-e8c2-4cce-921c-1f2fb6077d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = preprocess_image(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    prediction = model.predict(image)\n",
    "\n",
    "    print (prediction)\n",
    "    \n",
    "    # Vrátí třídu s nejvyšší pravděpodobností\n",
    "    class_labels = ['circle', 'heptagon', 'pentagon', 'square', 'star', 'triangle']  # Upravit kategorie (pozor na pořadí!)\n",
    "    predicted_class = np.argmax(prediction)\n",
    " \n",
    "    return class_labels[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18912433-ff12-4cea-b94c-753dda05e55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "[[9.9796665e-01 2.0334234e-03 5.9530782e-19 4.6548778e-38 0.0000000e+00\n",
      "  0.0000000e+00]]\n",
      "circle\n"
     ]
    }
   ],
   "source": [
    "result = predict_image(\"./predikce/unknown4.png\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae3e22-5637-40b7-a383-6680a025b20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f387450-3b59-4522-9293-2b3baa97df20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc5af23-55d0-4ed5-a21f-a1228e8428a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
